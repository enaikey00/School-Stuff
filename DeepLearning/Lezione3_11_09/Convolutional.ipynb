{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5595f2e-c757-45be-ae1c-1c08cbf49241",
   "metadata": {},
   "source": [
    "## Reti Convoluzionali"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc7d7cb-273f-4f08-b270-e3715f580262",
   "metadata": {},
   "source": [
    "### Filtro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421902d9-6483-4e38-ab37-a5c9e9aeb700",
   "metadata": {},
   "source": [
    "Concetto di Filtro: vedi appunti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7cdf62-52ba-453c-95c5-3635781b9a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "Link: https://setosa.io/ev/image-kernels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3444b993-4272-4bd3-8a47-de2f6804617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vedi anche 3Blue1brown serie video su NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5888121-4f1d-419a-be8d-341e1c1e7a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91954e14-e332-48d8-8d93-21bf0a574ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.convolutional.conv2d.Conv2D at 0x7f3126d67a00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=3, # solo 3 perché filtro quadrato\n",
    "    strides = 2, # di quanto shiftare a sx-dx e up-down, il passo/salto\n",
    "    # lo stride riduce l'output (ex. 2 dimezza le dimensioni dell'output: le sovrapposizioni diventano la metà, rispetto stride=1)\n",
    "    padding=\"same\",\n",
    "    activation = \"relu\" # serve sempre una attivazione (a meno dell'ultimo layer)\n",
    ")\n",
    "# quanti filtri, stride, padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb1d940-e1a6-4d36-8d35-4054d624d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# una immagine a colori ha 3 conali: 2 dim spaziali e 3 colori (una dimensione)\n",
    "# quindi il tensore di partenza (a meno di immagini B&W) a 4 dimensioni:\n",
    "\n",
    "batch_size x 640 x 480 x 3 (canali)\n",
    "# vedi anche appunti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cd92f9f-fce5-4f5a-8d10-50d2a4f54a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd82708-82f0-4276-952a-0c6372c38da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=3,\n",
    "    strides = 1,\n",
    "    padding=\"same\",\n",
    "    activation = \"relu\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f76ce1a0-f73b-41fe-821b-eee0850883dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch = tf.random.normal([32, 28, 28, 3])\n",
    "# batch di 32 immagini, ognuna 28x28, a colori (il filtro è 3x3x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70d57b4b-e229-458f-b9a5-c1fc288d250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = conv2d(img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee847b5b-1f12-45d9-b335-0a87f4a5bcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 28, 28, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# la dimensione di res: 32 elementi, ciascuno 28x28, con 64 canali (perché applico 64 filtri a ogni immagine)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bee9f1f-6a81-43bb-b99c-b206f267f6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1728, 64]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parametri (vedi appunti)\n",
    "[w.size for w in conv2d.get_weights()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a950f0a-8520-47c9-b79c-2ac4bda85c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1728 + 64 è il numero dei parametri dello strato conv2d\n",
    "# da dove derivano:\n",
    "vedi appunti\n",
    "\n",
    "# il prossimo strato ottiene qualcosa 28x28 con 64 canali o colori\n",
    "# filtro 3x3 x 64\n",
    "vedi appunti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be9c05-76cf-4021-9254-2df3c2abb493",
   "metadata": {},
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84090245-ab0a-433f-87c6-09473c91ece4",
   "metadata": {},
   "source": [
    "Rete Convo = Filtro + Pooling (vedi appunti)\n",
    "+ Il filtro (layer convoluzionale) vede le features\n",
    "+ Il pooling riduce le dimensioni dell'immagine, così che i filtri successivi, anche essendo della stessa shape, vedano una porzione più grande dell'immagine\n",
    "\n",
    "Quale differenza c'è fra i due? (in quello che fanno)\n",
    "+ [link 1](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/#:~:text=Pooling%20involves%20selecting%20a%20pooling,a%20stride%20of%202%20pixels.)\n",
    "+ https://towardsdatascience.com/understanding-convolutions-and-pooling-in-neural-networks-a-simple-explanation-885a2d78f211"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa554d-b975-4ded-b667-3d8433330361",
   "metadata": {},
   "source": [
    "### LeNet5 implementation\n",
    "per la rete vedi wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff811fc2-0d30-41ed-b079-770d7dd0441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Reshape, Lambda\n",
    "from keras import Sequential\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2195be-aa95-4da4-883c-eaa80af86b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Lambda Layer\n",
    "Strato con Stato Interno vuoto, state-less (non ha parametri interni, ex. il flatten, fa solo una operazione). \n",
    "Un dense layer è state-full.\n",
    "\n",
    "Lambda(lambda x: tf.reshape(x, shape=[28,28,1]) # può essere messo al posto della strato Reshape (fa la stessa cosa)\n",
    "# ingloba una funzione e ne fa uno strato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d6d8ef-d85a-45b2-bfbe-b51ed154b948",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet5 = Sequential(\n",
    "    [\n",
    "        Reshape(target_shape = (28,28,1), input_shape=(28,28)),\n",
    "        Conv2D(filters = 6, kernel_size=5, padding=\"same\",activation=\"relu\" ), \n",
    "        # cos'è kernel size? la dimensione del filtro: 5x5\n",
    "        MaxPooling2D(), # di default stride = pool_size\n",
    "        Conv2D(filters = 16, kernel_size=5, padding=\"valid\",activation=\"relu\" ), # c'è un motivo per i 16 filtri?\n",
    "        MaxPooling2D(),\n",
    "        Flatten(), # perché? perché devo passare l'ouput della convo. all'MLP come singolo vettore?\n",
    "        #          https://towardsdatascience.com/the-most-intuitive-and-easiest-guide-for-convolutional-neural-network-3607be47480#:~:text=Flattening%20is%20converting%20the%20data,called%20a%20fully%2Dconnected%20layer.\n",
    "        # ora MLP\n",
    "        Dense(120, \"relu\"),\n",
    "        Dense(84, \"relu\"),\n",
    "        Dense(10) # o nulla o softmax (output della rete, attento ai logits o probabilità e quindi alla funzione di loss)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a96744d-db95-4299-bd02-e592c11ca011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 6)         156       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 6)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 16)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 400)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               48120     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61706 (241.04 KB)\n",
      "Trainable params: 61706 (241.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lenet5.summary() # sproporzione pesi tra parte Convo e Densa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22b54d23-ee7e-41ec-915c-10578d05f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd05bb9c-9c77-4c88-b01f-65b1f737cde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /home/naska/miniconda3/lib/python3.9/site-packages (0.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9bbf143-4455-4769-acbf-20e0cc8b1598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(lenet5, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748456f4-5f21-4120-aa90-486c4bb24b1f",
   "metadata": {},
   "source": [
    "#### Applica al mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac7d07a3-06cd-4f84-9f31-49b58e210b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets.mnist import load_data\n",
    "import numpy as np\n",
    "from keras.losses  import SparseCategoricalCrossentropy\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eb35f71-8eea-4edf-bfa5-f3a1be3d3c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "715c0d1c-c8da-44d7-a436-5c90999d7293",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(np.float32) / 255\n",
    "x_test = x_test.astype(np.float32) / 255\n",
    "\n",
    "y_train = y_train.astype(np.float32) # metti int32\n",
    "y_test = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74139f10-8970-4373-9187-d25a750173c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet5.compile(\n",
    "    optimizer=Adam(), \n",
    "    loss = SparseCategoricalCrossentropy(from_logits = True), \n",
    "    metrics=[\"Accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "380238d8-13aa-4efa-9f9c-e42d14cc2092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1500/1500 [==============================] - 26s 16ms/step - loss: 0.2117 - Accuracy: 0.9360 - val_loss: 0.0771 - val_Accuracy: 0.9766\n",
      "Epoch 2/20\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0672 - Accuracy: 0.9795 - val_loss: 0.0617 - val_Accuracy: 0.9816\n",
      "Epoch 3/20\n",
      "1500/1500 [==============================] - 25s 16ms/step - loss: 0.0480 - Accuracy: 0.9847 - val_loss: 0.0580 - val_Accuracy: 0.9827\n",
      "Epoch 4/20\n",
      "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0377 - Accuracy: 0.9881 - val_loss: 0.0476 - val_Accuracy: 0.9854\n",
      "Epoch 5/20\n",
      "1500/1500 [==============================] - 23s 16ms/step - loss: 0.0302 - Accuracy: 0.9905 - val_loss: 0.0522 - val_Accuracy: 0.9844\n",
      "Epoch 6/20\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0247 - Accuracy: 0.9919 - val_loss: 0.0508 - val_Accuracy: 0.9853\n",
      "Epoch 7/20\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0220 - Accuracy: 0.9929 - val_loss: 0.0467 - val_Accuracy: 0.9869\n",
      "Epoch 8/20\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0166 - Accuracy: 0.9949 - val_loss: 0.0571 - val_Accuracy: 0.9843\n",
      "Epoch 9/20\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0156 - Accuracy: 0.9947 - val_loss: 0.0570 - val_Accuracy: 0.9849\n",
      "Epoch 10/20\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0140 - Accuracy: 0.9950 - val_loss: 0.0601 - val_Accuracy: 0.9851\n",
      "Epoch 11/20\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0129 - Accuracy: 0.9956 - val_loss: 0.0711 - val_Accuracy: 0.9845\n",
      "Epoch 12/20\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0118 - Accuracy: 0.9961 - val_loss: 0.0514 - val_Accuracy: 0.9881\n",
      "Epoch 13/20\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0095 - Accuracy: 0.9968 - val_loss: 0.0509 - val_Accuracy: 0.9898\n",
      "Epoch 14/20\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0079 - Accuracy: 0.9978 - val_loss: 0.0676 - val_Accuracy: 0.9863\n",
      "Epoch 15/20\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0102 - Accuracy: 0.9968 - val_loss: 0.0701 - val_Accuracy: 0.9858\n",
      "Epoch 16/20\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0102 - Accuracy: 0.9966 - val_loss: 0.0610 - val_Accuracy: 0.9867\n",
      "Epoch 17/20\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0069 - Accuracy: 0.9978 - val_loss: 0.0580 - val_Accuracy: 0.9883\n",
      "Epoch 18/20\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0073 - Accuracy: 0.9975 - val_loss: 0.0675 - val_Accuracy: 0.9873\n",
      "Epoch 19/20\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0077 - Accuracy: 0.9976 - val_loss: 0.0653 - val_Accuracy: 0.9872\n",
      "Epoch 20/20\n",
      "1500/1500 [==============================] - 23s 16ms/step - loss: 0.0072 - Accuracy: 0.9978 - val_loss: 0.0611 - val_Accuracy: 0.9886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f91c1378a60>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet5.fit(x = x_train, y= y_train, epochs = 20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb615ca-c739-49ef-a876-4e5ae6df7857",
   "metadata": {},
   "source": [
    "#### Commento\n",
    "+ MLP finale: nelle archit. moderne non c'è o è meno gigante\n",
    "+ Reti Convo anche per altri input (ex. elettrocardiogrammi...) (con dimensioni diverse)\n",
    "+ https://www.image-net.org/ dataset imm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c162ef54-6ecd-45a8-bb2c-1c8261cab83d",
   "metadata": {},
   "source": [
    "## Label Studio\n",
    "+ [link](https://labelstud.io/)\n",
    "+ labeling dei dati (di tutti i tipi)\n",
    "+ anche in team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a01ce-e7fb-4aba-b7f2-dad91bf2758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentazione Semantica: taggare pixel per pixel (ex. in ambito medico, tessuti ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701c3246-0226-4d5f-9343-08c79acc2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run label-studio: try it out\n",
    "# è possibile un pre-labeling con modelli di ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a823a1a-f189-4bec-827b-470b7b37e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usalo in un conda env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa412a2-bf18-41d5-9210-3aac00bd2d49",
   "metadata": {},
   "source": [
    "## BackPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97baa1e-75ea-4929-840e-127342a6dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vedi appunti"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
