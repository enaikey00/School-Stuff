{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codice della Lezione4 che abbiamo rivisto in Lezione5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "ngFxGdJprQKi",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-20 11:21:16.369183: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-20 11:21:19.971257: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "^C\n",
      "\n",
      "\u001b[31mAborted.\u001b[0m\n",
      "2023-09-20 11:22:08.840382: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-20 11:22:10.346976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/naska/miniconda3/lib/python3.9/runpy.py\", line 188, in _run_module_as_main\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
      "  File \"/home/naska/miniconda3/lib/python3.9/runpy.py\", line 147, in _get_module_details\n",
      "    return _get_module_details(pkg_main_name, error)\n",
      "  File \"/home/naska/miniconda3/lib/python3.9/runpy.py\", line 111, in _get_module_details\n",
      "    __import__(pkg_name)\n",
      "  File \"/home/naska/miniconda3/lib/python3.9/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/home/naska/miniconda3/lib/python3.9/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/home/naska/miniconda3/lib/python3.9/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/home/naska/miniconda3/lib/python3.9/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/home/naska/miniconda3/lib/python3.9/site-packages/thinc/config.py\", line 4, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/home/naska/miniconda3/lib/python3.9/site-packages/thinc/types.py\", line 8, in <module>\n",
      "    from .compat import has_cupy, cupy\n",
      "  File \"/home/naska/miniconda3/lib/python3.9/site-packages/thinc/compat.py\", line 54, in <module>\n",
      "    import tensorflow.experimental.dlpack\n",
      "  File \"/home/naska/miniconda3/lib/python3.9/site-packages/tensorflow/__init__.py\", line 478, in <module>\n",
      "    importlib.import_module(\"keras.optimizers\")\n",
      "  File \"/home/naska/miniconda3/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/home/naska/miniconda3/lib/python3.9/site-packages/keras/__init__.py\", line 3, in <module>\n",
      "    from keras import __internal__\n",
      "  File \"/home/naska/miniconda3/lib/python3.9/site-packages/keras/__internal__/__init__.py\", line 3, in <module>\n",
      "    from keras.__internal__ import backend\n",
      "  File \"/home/naska/miniconda3/lib/python3.9/site-packages/keras/__internal__/backend/__init__.py\", line 3, in <module>\n",
      "    from keras.src.backend import _initialize_variables as initialize_variables\n",
      "  File \"/home/naska/miniconda3/lib/python3.9/site-packages/keras/src/__init__.py\", line 21, in <module>\n",
      "    from keras.src import models\n",
      "  File \"/home/naska/miniconda3/lib/python3.9/site-packages/keras/src/models/__init__.py\", line 31, in <module>\n",
      "    from keras.src.models.sharpness_aware_minimization import SharpnessAwareMinimization\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 846, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 941, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1039, in get_data\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md\n",
    "!python -m spacy download it_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "62Ap-HTCpyMM",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 11:22:16.659109: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-20 11:22:18.151819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from tensorflow.data import Dataset\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import GRUCell, Embedding, Attention\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ps6Q2QbnmORQ",
    "outputId": "4d541dec-dc30-483d-96c9-9fad0484d3ca"
   },
   "outputs": [],
   "source": [
    "!wget https://www.manythings.org/anki/ita-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W3KENBgWo2Jz",
    "outputId": "4ee1ffca-f0c7-44b4-a73d-a4128525ba97"
   },
   "outputs": [],
   "source": [
    "!unzip ita-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wesJLDoVWX3"
   },
   "outputs": [],
   "source": [
    "UNK = \"<UNK>\"\n",
    "BOS = \"<BOS>\"\n",
    "EOS = \"<EOS>\"\n",
    "PAD = \"<PAD>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpyzYDyt3dq2"
   },
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BOS_IDX = 2\n",
    "EOS_IDX = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4tZPWUFpqdK"
   },
   "source": [
    "# tokenizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aw9S760lprjM"
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, language):\n",
    "        self.language = language\n",
    "        if language == \"it\":\n",
    "            self.nlp = spacy.load(\"it_core_news_md\")\n",
    "        else:\n",
    "            self.nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return [token.text.lower() for token in self.nlp(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyZ2ZBjAr-cR"
   },
   "source": [
    "# vocabolario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCcyydqAsEBo"
   },
   "outputs": [],
   "source": [
    "def build_vocabularies(filepath, num_rows, vocab_size):\n",
    "\n",
    "    # counter\n",
    "    en_counter = Counter()\n",
    "    it_counter = Counter()\n",
    "\n",
    "    # tokenizzatori\n",
    "    en_tokenizer = Tokenizer(\"en\")\n",
    "    it_tokenizer = Tokenizer(\"it\")\n",
    "\n",
    "    # vocabolari\n",
    "    en_vocab = {\"<PAD>\": 0, \"<UNK>\": 1, \"<BOS>\": 2, \"<EOS>\": 3}\n",
    "    it_vocab = {\"<PAD>\": 0, \"<UNK>\": 1, \"<BOS>\": 2, \"<EOS>\": 3}\n",
    "\n",
    "    counter = 0\n",
    "    pbar = tqdm()\n",
    "    with open(filepath, \"r\") as f:\n",
    "        while (line := f.readline()) is not None:\n",
    "            en_sentence, it_sentence, *_ = line.split(\"\\t\")\n",
    "            en_tokenized = en_tokenizer.tokenize(en_sentence)\n",
    "            it_tokenized = it_tokenizer.tokenize(it_sentence)\n",
    "\n",
    "            # update dei Counter\n",
    "            en_counter.update(en_tokenized)\n",
    "            it_counter.update(it_tokenized)\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "            counter += 1\n",
    "            if num_rows and counter >= num_rows:\n",
    "                break\n",
    "\n",
    "    # update dei dizionari\n",
    "    most_common_en = [item[0] for item in en_counter.most_common(vocab_size)]\n",
    "    most_common_it = [item[0] for item in it_counter.most_common(vocab_size)]\n",
    "\n",
    "    en_vocab |= {word: idx for idx, word in enumerate(most_common_en, start=4)}\n",
    "    it_vocab |= {word: idx for idx, word in enumerate(most_common_it, start=4)}\n",
    "\n",
    "    en_vocab_inv = {idx: word for word, idx in en_vocab.items()}\n",
    "    it_vocab_inv = {idx: word for word, idx in it_vocab.items()}\n",
    "\n",
    "    return en_vocab, en_vocab_inv, it_vocab, it_vocab_inv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oX04zDv_v0HV"
   },
   "outputs": [],
   "source": [
    "en_vocab, en_vocab_inv, it_vocab, it_vocab_inv = build_vocabularies(filepath=\"ita.txt\", num_rows=100_000, vocab_size=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GTiO_3AX4Nw"
   },
   "source": [
    "# Dump vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2rZ0BuxBYGUz"
   },
   "outputs": [],
   "source": [
    "with open(\"en_vocab.json\", \"w\") as f:\n",
    "    json.dump(en_vocab, f)\n",
    "\n",
    "with open(\"en_vocab_inv.json\", \"w\") as f:\n",
    "    json.dump(en_vocab_inv, f)\n",
    "\n",
    "with open(\"it_vocab.json\", \"w\") as f:\n",
    "    json.dump(it_vocab, f)\n",
    "\n",
    "with open(\"it_vocab_inv.json\", \"w\") as f:\n",
    "    json.dump(it_vocab_inv, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Usi1BDxCX6d7"
   },
   "source": [
    "# Load vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyVlzwQQ-GDS"
   },
   "outputs": [],
   "source": [
    "with open(\"en_vocab.json\") as f:\n",
    "    en_vocab = json.load(f)\n",
    "\n",
    "with open(\"en_vocab_inv.json\") as f:\n",
    "    en_vocab_inv = json.load(f)\n",
    "\n",
    "with open(\"it_vocab.json\") as f:\n",
    "    it_vocab = json.load(f)\n",
    "\n",
    "with open(\"it_vocab_inv.json\") as f:\n",
    "    it_vocab_inv = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRTQP0A50T8z"
   },
   "outputs": [],
   "source": [
    "it_vocab_inv = {int(k): v for k, v in it_vocab_inv.items()}\n",
    "en_vocab_inv = {int(k): v for k, v in en_vocab_inv.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ap3gBed41Ysh"
   },
   "source": [
    "# split train-validation-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIRsEu7c1j1W"
   },
   "outputs": [],
   "source": [
    "def split_file(filepath):\n",
    "    with open(filepath, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    num_rows = len(lines)\n",
    "    shuffled_row_idxs = random.sample(list(range(num_rows)), k=num_rows)\n",
    "\n",
    "    train_idxs = shuffled_row_idxs[:int(num_rows*0.6)]\n",
    "    val_idxs = shuffled_row_idxs[int(num_rows*0.6):int(num_rows*0.8)]\n",
    "    test_idxs = shuffled_row_idxs[int(num_rows*0.8):]\n",
    "\n",
    "    # train\n",
    "    with open(\"train.txt\", \"w\") as f:\n",
    "        f.writelines([lines[idx] for idx in train_idxs])\n",
    "\n",
    "    # validation\n",
    "    with open(\"val.txt\", \"w\") as f:\n",
    "        f.writelines([lines[idx] for idx in val_idxs])\n",
    "\n",
    "    # test\n",
    "    with open(\"test.txt\", \"w\") as f:\n",
    "        f.writelines([lines[idx] for idx in test_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nANyIJ533-lC"
   },
   "outputs": [],
   "source": [
    "split_file(\"ita.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0kWmX3b-C-3"
   },
   "source": [
    "# Generatori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xahh-257-QKj"
   },
   "outputs": [],
   "source": [
    "def dataset_generator(filepath, en_dict, it_dict, en_tokenizer, it_tokenizer):\n",
    "    def gen():\n",
    "        with open(filepath, \"r\") as f:\n",
    "            while (line := f.readline()) is not None:\n",
    "                en_sentence, it_sentence, *_ = line.split(\"\\t\")\n",
    "                en_sentence_tokenized = en_tokenizer.tokenize(en_sentence)\n",
    "                it_sentence_tokenized = it_tokenizer.tokenize(it_sentence)\n",
    "                src_sentence_tokenized = en_sentence_tokenized + [EOS]\n",
    "                tgt_sentence_in_tokenized = [BOS] + it_sentence_tokenized\n",
    "                tgt_sentence_out_tokenized = it_sentence_tokenized.copy() + [EOS]\n",
    "\n",
    "                src_sentence_encoded = [en_dict.get(token, en_dict[UNK]) for token in src_sentence_tokenized]\n",
    "                tgt_sentence_in_encoded = [it_dict.get(token, it_dict[UNK]) for token in tgt_sentence_in_tokenized]\n",
    "                tgt_sentence_out_encoded = [it_dict.get(token, it_dict[UNK]) for token in tgt_sentence_out_tokenized]\n",
    "\n",
    "                yield (src_sentence_encoded, tgt_sentence_in_encoded), tgt_sentence_out_encoded\n",
    "\n",
    "    return gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdd7bvBmYWjA"
   },
   "outputs": [],
   "source": [
    "# for (src, tgt_in), tgt_out in dataset_generator(\"val.txt\", en_vocab, it_vocab, Tokenizer(\"en\"), Tokenizer(\"it\")):\n",
    "#     print(src)\n",
    "#     print(tgt_in)\n",
    "#     print(tgt_out)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpgWbHFvYxDP"
   },
   "outputs": [],
   "source": [
    "trainset = Dataset.from_generator(\n",
    "    generator=dataset_generator(\"train.txt\", en_vocab, it_vocab, Tokenizer(\"en\"), Tokenizer(\"it\")),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(None,), dtype=tf.int32), tf.TensorSpec(shape=(None,), dtype=tf.int32)),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32))\n",
    ")\n",
    "\n",
    "valset = Dataset.from_generator(\n",
    "    generator=dataset_generator(\"val.txt\", en_vocab, it_vocab, Tokenizer(\"en\"), Tokenizer(\"it\")),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(None,), dtype=tf.int32), tf.TensorSpec(shape=(None,), dtype=tf.int32)),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32))\n",
    ")\n",
    "\n",
    "testset = Dataset.from_generator(\n",
    "    generator=dataset_generator(\"test.txt\", en_vocab, it_vocab, Tokenizer(\"en\"), Tokenizer(\"it\")),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(None,), dtype=tf.int32), tf.TensorSpec(shape=(None,), dtype=tf.int32)),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCWAwZtFbio_"
   },
   "outputs": [],
   "source": [
    "trainset = trainset.shuffle(buffer_size=1000, reshuffle_each_iteration=True)\n",
    "trainset = trainset.padded_batch(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EklA1mMxwMae"
   },
   "outputs": [],
   "source": [
    "class TextEncoderDecoder:\n",
    "    def __init__(self, en_vocab, en_vocab_inv, it_vocab, it_vocab_inv):\n",
    "        self.en_vocab = en_vocab\n",
    "        self.en_vocab_inv = en_vocab_inv\n",
    "        self.it_vocab = it_vocab\n",
    "        self.it_vocab_inv = it_vocab_inv\n",
    "        self.nlp_it = spacy.load(\"it_core_news_md\")\n",
    "        self.nlp_en = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "    def encode(self, text, language):\n",
    "        vocab = self.en_vocab if language == \"en\" else self.it_vocab\n",
    "        nlp = self.nlp_en if language == \"en\" else self.nlp_it\n",
    "        tokenized_text = [token.text.lower() for token in nlp(text)]\n",
    "        return [vocab.get(token, vocab[UNK]) for token in tokenized_text]\n",
    "\n",
    "    def decode(self, coded_text, language):\n",
    "        vocab_inv = self.en_vocab_inv if language == \"en\" else self.it_vocab_inv\n",
    "        return [vocab_inv[code] for code in coded_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7SO2N9jf7tx"
   },
   "source": [
    "# Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pyk5t36rbk3"
   },
   "outputs": [],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, vocabulary_size, embedding_size, recurrent_layers, recurrent_units, **kwargs):\n",
    "        \"\"\"\n",
    "        args\n",
    "        ----\n",
    "        - vocabulary_size (int): including special tokens (<BOS>, <EOS>, <UNK>)\n",
    "        - embedding_size (int): dimensione dello spazio degli embedding\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # embedding\n",
    "        # 0 index mean padding\n",
    "        self.embedding = Embedding(\n",
    "            vocabulary_size, embedding_size, mask_zero=True)\n",
    "\n",
    "        gru_cells = [GRUCell(recurrent_units) for _ in range(recurrent_layers)]\n",
    "\n",
    "        stacked_cells = tf.keras.layers.StackedRNNCells(gru_cells)\n",
    "        self.gru_layer = tf.keras.layers.RNN(stacked_cells, return_state=True, return_sequences=True)\n",
    "\n",
    "    def call(self, data, training=None):\n",
    "        x = self.embedding(data, training=training)\n",
    "        output, *state = self.gru_layer(x, training=training)\n",
    "\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rG0MTvBGrxz2"
   },
   "outputs": [],
   "source": [
    "class Decoder(keras.Model):\n",
    "    def __init__(self, vocabulary_size, embedding_size, recurrent_layers, recurrent_units, attention=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.recurrent_layers = recurrent_layers\n",
    "        self.recurrent_units = recurrent_units\n",
    "\n",
    "        self.embedding = Embedding(\n",
    "            vocabulary_size, embedding_size, mask_zero=True)\n",
    "\n",
    "        gru_cells = [GRUCell(recurrent_units) for _ in range(recurrent_layers)]\n",
    "\n",
    "        stacked_cells = tf.keras.layers.StackedRNNCells(gru_cells)\n",
    "        self.gru_layer = tf.keras.layers.RNN(stacked_cells, return_sequences=True, return_state=True)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(vocabulary_size)\n",
    "\n",
    "        if attention:\n",
    "            self.attention = Attention(score_mode=\"dot\")\n",
    "\n",
    "    def call(self, target_in, encoder_output, encoder_state, training=None, max_sentence_length=None):\n",
    "        # target_in.shape = batch x length\n",
    "        # initial_state.shape = batch x recurrent_layers x recurrent_units\n",
    "        # encoder_output.shape = batch x length x recurrent_units\n",
    "\n",
    "        # Addestramento\n",
    "        x = self.embedding(target_in, training=training)\n",
    "        # data.shape = batch x length x embedding\n",
    "        # concatena il contesto con l'input\n",
    "        # x = tf.concat([x, tf.repeat(tf.expand_dims(encoder_output[:, -1, :], axis=1), repeats=x.shape[1], axis=1)], axis=2)\n",
    "\n",
    "        output, *state = self.gru_layer(x, training=training, initial_state=encoder_state)\n",
    "        return tf.keras.activations.softmax(self.dense(output)), state\n",
    "\n",
    "    def generate(self, encoder_output, encoder_state, training=None, max_sentence_length=None):\n",
    "        # Generazione\n",
    "        batch_size = encoder_output.shape[0]\n",
    "        x = tf.fill([batch_size, 1], BOS_IDX)\n",
    "        state = encoder_state\n",
    "        out_words_list = []\n",
    "        for _ in range(max_sentence_length):\n",
    "            # x.shape = batch_size x 1 x embedding_size\n",
    "            x = self.embedding(x, training=training)\n",
    "            # aggiunge il contesto\n",
    "            # x = tf.concat([x, tf.expand_dims(encoder_output[:, -1, :], axis=1)], axis=2)\n",
    "            output, *state = self.gru_layer(x, training=training, initial_state=state)\n",
    "            # output_size = batch_size x 1 x embedding_size\n",
    "\n",
    "            # trova i caratteri più probabili\n",
    "            # probs.shape = batch_size x 1 x vocabulary_size\n",
    "            probs = tf.keras.activations.softmax(self.dense(output))\n",
    "            x = tf.argmax(probs, axis=-1)\n",
    "            out_words_list.append(x.numpy().item())\n",
    "\n",
    "        return out_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyqDnFlc3Cpv"
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6_9ew1pr0jm"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(keras.Model):\n",
    "    def __init__(self, vocabulary_size, embedding_size, recurrent_layers, recurrent_units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.recurrent_layers = recurrent_layers\n",
    "        self.recurrent_units = recurrent_units\n",
    "\n",
    "\n",
    "        self.encoder = Encoder(vocabulary_size, embedding_size,\n",
    "                               recurrent_layers, recurrent_units)\n",
    "        self.decoder = Decoder(vocabulary_size, embedding_size, recurrent_layers,\n",
    "                               recurrent_units)\n",
    "\n",
    "\n",
    "    def call(self, data, training=None, max_sentence_length=MAX_SENTENCE_LENGTH):\n",
    "        # unpack data\n",
    "        src_sentences, dst_sentences = data\n",
    "\n",
    "        # encoder call\n",
    "        # encoder_output.shape = batch x len_sentences x encoder_recurrent_units\n",
    "        encoder_output, encoder_state = self.encoder(src_sentences, training=training)\n",
    "\n",
    "        decoder_output, decoder_state = self.decoder(dst_sentences, encoder_output, encoder_state, training=training, max_sentence_length=max_sentence_length)\n",
    "\n",
    "        return decoder_output\n",
    "\n",
    "    def generate(self, data, training=None, max_sentence_length=MAX_SENTENCE_LENGTH):\n",
    "        # unpack data\n",
    "        src_sentences = data\n",
    "\n",
    "        # encoder call\n",
    "        # encoder_output.shape = batch x len_sentences x encoder_recurrent_units\n",
    "        encoder_output, encoder_state = self.encoder(src_sentences, training=training)\n",
    "\n",
    "        words = self.decoder.generate(encoder_output, encoder_state, training=training, max_sentence_length=max_sentence_length)\n",
    "\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYBHzwjapKGt"
   },
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    y_true_reshaped = tf.reshape(y_true, [-1])\n",
    "    y_pred_reshaped = tf.reshape(y_pred, [-1, y_pred.shape[-1]])\n",
    "    scc = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        reduction=tf.keras.losses.Reduction.NONE)\n",
    "    results = scc(y_true_reshaped, y_pred_reshaped)\n",
    "    mask = tf.cast(y_true_reshaped != 0, tf.float32)\n",
    "    return tf.reduce_sum(results*mask) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdTs4kiC36CO"
   },
   "source": [
    "# Addestramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcSDNyEv38uv"
   },
   "outputs": [],
   "source": [
    "encoder_decoder = EncoderDecoder(vocabulary_size=10_000+4, embedding_size=128, recurrent_layers=1, recurrent_units=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYp2P-Pm-qCx"
   },
   "outputs": [],
   "source": [
    "encoder_decoder.compile(optimizer=keras.optimizers.Adam(), loss=custom_loss, run_eagerly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tOGJLwh--2vC",
    "outputId": "f03c5237-d05e-4709-aa7b-89cfed7d7e53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100\n",
      "100/100 [==============================] - 68s 541ms/step - loss: 1.9064\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 57s 571ms/step - loss: 1.9124\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 54s 541ms/step - loss: 1.8792\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 54s 537ms/step - loss: 1.9470\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 55s 547ms/step - loss: 2.0461\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 54s 544ms/step - loss: 2.0240\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 54s 542ms/step - loss: 2.0119\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 56s 558ms/step - loss: 1.9586\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 54s 543ms/step - loss: 1.9464\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 55s 550ms/step - loss: 1.9473\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 54s 542ms/step - loss: 2.0492\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 55s 546ms/step - loss: 2.0509\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 54s 544ms/step - loss: 2.0464\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 55s 546ms/step - loss: 2.0415\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 55s 549ms/step - loss: 1.9825\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 55s 547ms/step - loss: 1.9727\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 54s 537ms/step - loss: 2.0010\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 54s 538ms/step - loss: 1.9459\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 55s 549ms/step - loss: 1.9722\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 55s 546ms/step - loss: 2.0404\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 54s 539ms/step - loss: 2.0433\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 56s 556ms/step - loss: 1.9937\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 54s 536ms/step - loss: 2.0184\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 53s 534ms/step - loss: 2.0085\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 53s 532ms/step - loss: 1.9741\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 53s 528ms/step - loss: 1.9564\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 52s 524ms/step - loss: 1.9449\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 54s 541ms/step - loss: 1.9679\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 52s 524ms/step - loss: 1.9709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f7c1a76a470>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_decoder.fit(x=trainset, steps_per_epoch=100, epochs=100, initial_epoch=71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKPQq5eloQwY"
   },
   "outputs": [],
   "source": [
    "encoder_decoder.save_weights(\"weights_100.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Pk6iNeGhZGz"
   },
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvdCSGuS-89a",
    "outputId": "7a0a0016-ecfb-4db1-a5e2-d3e2305a9eb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sono un insegnante di francese , non sono bravo in un amico . <EOS> . <EOS> . <EOS> . <EOS>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"i am a good guy and my home is beautiful\"\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "tokens = [t.text.lower() for t in nlp(sentence)]\n",
    "idxs = [en_vocab.get(t, en_vocab[UNK]) for t in tokens]\n",
    "translation = encoder_decoder.generate(tf.reshape(tf.constant(idxs), [1, -1]))\n",
    "print(\" \".join([it_vocab_inv[idx] for idx in translation]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
