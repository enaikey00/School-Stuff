{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c78ac9-89cd-443c-b0ba-2d114c079013",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334130b6-39a7-4586-9f39-4c8782782359",
   "metadata": {},
   "outputs": [],
   "source": [
    "vedi appunti x teoria\n",
    "vedi Lezione6 -> Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6ddb1b-68f1-481b-b8c6-47b5ce29a9a1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-25 10:15:46.615965: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-25 10:15:47.054130: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-25 10:15:47.057485: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-25 10:15:49.234933: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd0312-e7ac-4afc-a52e-280504677490",
   "metadata": {},
   "source": [
    "## BERT -- Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb9aa50-d9b5-4ba7-b6a1-8cc103aac3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vedi appunti x teoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec52bf34-19c9-47ef-b5c3-661993d0c546",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Hugging Face\n",
    "vedi file prof. \"huggingface\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db011a56-6127-42a4-aa69-be497ba6a6d1",
   "metadata": {},
   "source": [
    "+ repository di modelli (checkpoints)\n",
    "+ vari task (NLP, ComputerVision, ...)\n",
    "+ tanti dataset\n",
    "+ librerie di HF\n",
    "+ https://huggingface.co/docs/transformers/index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12175f7-7276-45ce-9ccc-bffad3fc3d6f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba91b42-f447-4759-850b-09b8e962ea32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /home/naska/miniconda3/lib/python3.9/site-packages (0.13.3)\n",
      "Requirement already satisfied: transformers in /home/naska/miniconda3/lib/python3.9/site-packages (4.29.2)\n",
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /home/naska/miniconda3/lib/python3.9/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/naska/miniconda3/lib/python3.9/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/naska/miniconda3/lib/python3.9/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/naska/miniconda3/lib/python3.9/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/naska/miniconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/naska/miniconda3/lib/python3.9/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in /home/naska/miniconda3/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/naska/miniconda3/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Obtaining dependency information for pyarrow>=8.0.0 from https://files.pythonhosted.org/packages/49/db/0a40d2a5b2382c77536479894ce2900e5f4c40251681a72d397ba6430f8d/pyarrow-13.0.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading pyarrow-13.0.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/naska/miniconda3/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/naska/miniconda3/lib/python3.9/site-packages (from datasets) (1.5.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/45/63/40da996350689cf29db7f8819aafa74c9d36feca4f0e4393d220c619a1dc/xxhash-3.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading xxhash-3.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/c6/c9/820b5ab056f4ada76fbe05bd481a948f287957d6cbfd59e2dd2618b408c1/multiprocess-0.70.15-py39-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /home/naska/miniconda3/lib/python3.9/site-packages (from datasets) (2023.5.0)\n",
      "Requirement already satisfied: aiohttp in /home/naska/miniconda3/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/naska/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/naska/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/naska/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/naska/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/naska/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/naska/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/naska/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/naska/miniconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/naska/miniconda3/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/naska/miniconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/naska/miniconda3/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Obtaining dependency information for dill<0.3.8,>=0.3.0 from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/naska/miniconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/naska/miniconda3/lib/python3.9/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/naska/miniconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pyarrow-13.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m568.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m400.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m615.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.8/193.8 kB\u001b[0m \u001b[31m531.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.6\n",
      "    Uninstalling dill-0.3.6:\n",
      "      Successfully uninstalled dill-0.3.6\n",
      "Successfully installed datasets-2.14.5 dill-0.3.7 multiprocess-0.70.15 pyarrow-13.0.0 xxhash-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a883c6-059b-4777-9c0c-9b7da94f70bf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naska/miniconda3/envs/MachineLearning/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-09-25 17:07:14.008478: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-25 17:07:16.034117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from tensorflow import keras\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d96b78c-3822-4762-a837-c1b2373a2a0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Pipeline - Task di Question&Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd4e1f41-13ca-48f6-97ea-89c55f01dc24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading model.safetensors: 100%|██████████████████████████████████████████████████████████| 261M/261M [00:23<00:00, 11.2MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|████████████████████████████████████████████████████| 29.0/29.0 [00:00<00:00, 79.1kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|█████████████████████████████████████████████████████| 213k/213k [00:00<00:00, 924kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|████████████████████████████████████████████████████| 436k/436k [00:00<00:00, 1.11MB/s]\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(task = \"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74919d1c-5203-4dc3-a7cc-5e66226229b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9945299029350281, 'start': 29, 'end': 34, 'answer': 'Milan'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(context = \"Rizzoli is a school based in Milan\", question = \"Where is Rizzoli?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f006e4fa-86e0-4166-872a-0ce6e18e0a58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Task di Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e31b96b-f108-47f3-8b65-985ed208c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db412e1-3ded-4c4c-b32d-e5ae0fedea85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(path = \"emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "266021a6-8ba9-4d81-933f-f87237948838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"] # i dataset sono caricati un pezzo alla volta, o anche in streaming da online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5f29eb1-5d6c-48da-932f-70db2f88d384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i didnt feel humiliated', 'label': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d434aea-3c9a-4887-809a-44f0343fa201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "204cf8ee-6789-4618-bb47-4516b0960617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.features[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f04e60f8-60db-49ee-8499-7c53a520981c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['i didnt feel humiliated',\n",
       "  'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n",
       "  'im grabbing a minute to post i feel greedy wrong',\n",
       "  'i am ever feeling nostalgic about the fireplace i will know that it is still on the property'],\n",
       " 'label': [0, 0, 3, 2]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:4] # estrae 4 elementi, 4 features e 4 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c45446d-0c27-4fb1-9344-c1983d99bebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                            i didnt feel humiliated      0\n",
       "1  i can go from feeling so hopeless to so damned...      0\n",
       "2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3  i am ever feeling nostalgic about the fireplac...      2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.set_format(\"pandas\") # è un'opzione\n",
    "train_dataset[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe98ad61-f5a8-44f6-a459-a0ecbc4807b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ba5df7c-3532-426a-990e-cf961172e189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████| 16000/16000 [00:01<00:00, 15640.42 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 15435.56 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 14936.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# label_descr: nuova colonna del dataset \n",
    "# dataset.map() prende un sample e lo trasforma\n",
    "# in HF un sample è sempre un dict. -- text : label\n",
    "\n",
    "# nuovo dataset\n",
    "int2str = train_dataset.features[\"label\"].int2str\n",
    "lambda x: dict( label_descr = int2str(x[\"label\"]) )\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda x: dict( label_descr = train_dataset.features[\"label\"].int2str(x[\"label\"]) ) \n",
    ")\n",
    "\n",
    "Nota: {a : 1} è stesso che dict(a=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87f0bcc8-88c3-45e2-a898-777924e96900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_descr'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'label_descr'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'label_descr'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "842560da-48b4-49df-9801-069be4393fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_descr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_descr\n",
       "0                            i didnt feel humiliated      0     sadness\n",
       "1  i can go from feeling so hopeless to so damned...      0     sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong      3       anger\n",
       "3  i am ever feeling nostalgic about the fireplac...      2        love\n",
       "4                               i am feeling grouchy      3       anger"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.set_format(\"pandas\")\n",
    "dataset[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1551445b-f55a-4a3b-98de-ab396fdb9cc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Classificazione con Bert: Sentiment Analysis\n",
    "modello pre-addestrato e poi fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd6eb1e3-76c2-49e4-8ea5-261ffae5d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoConfig, AutoModel, TFAutoModel, AutoTokenizer \n",
    "# TFAutomodel per scaricare la versione tensorflow dello stesso modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8413fdb1-5d3d-4bc3-84ea-8aa74b6576ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "# Nota: scarica architettura + pesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28682704-adbf-437e-9b46-5ae8fa185a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c92f565e-dbb2-4e2a-b0d0-2a3a1cefd074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.bert.modeling_tf_bert.TFBertModel at 0x7fcda7a1c910>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14c564ff-7820-4ccf-a580-be7a3a9212b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(model, keras.Model) # il modello è una sottoclasse di un modello di Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c0770e9-035f-4b89-ae75-21c3fb319e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,482,240\n",
      "Trainable params: 109,482,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5bf81a3-0f59-48a0-bc5e-cf393b97d5ee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pt = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "model_pt # outputs l'architettura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c83427-891a-49da-9cb7-d340f970a44d",
   "metadata": {},
   "source": [
    "Nota:\n",
    "il modello è composto da due parti\n",
    "+ body: modello base\n",
    "+ head: aggiunta, dipende dal task\n",
    "\n",
    "l'**head** è la parte da allenare per il nostro task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7049df60-7f4f-4fbe-89ad-c1f377f6cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25048768-7205-41ee-b24a-133316eeafa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "# scarica il body di BERT, allenato per il suo task originale, e poi adds a head for SequenceClassification, che è da allenare\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29ddfb46-b0f0-44f8-be33-e2acadbf7d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)/main/tokenizer.json: 100%|████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 2.08MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") \n",
    "# Nota: deve ess. lo stesso Tokenizzatore usato in fase di Training\n",
    "# ogni Checkpoint ha il suo tokenizzatore -> download con AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "605e86a5-bcd7-439e-b699-63edc25d1aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 1005, 1049, 2200, 2204, 2012, 2394, 102], [101, 1045, 1005, 1049, 2182, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    [\n",
    "     \"I'm very good at english\", \n",
    "     \"I'm here\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e351c274-acda-4461-b83f-f5743d2ee204",
   "metadata": {},
   "source": [
    "**Nota**\n",
    "+ il tokenizer fa un padding delle parole più corte: attention_mask, se ha degli zeri -> c'è padding\n",
    "+ token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48ec4d7d-3a03-4965-a134-63e06dbb69d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 1005, 1049, 2200, 2204, 2012, 2394, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] i'm very good at english [SEP]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer(\"I'm very good at english\")\n",
    "print(encoded)\n",
    "\n",
    "tokens = tokenizer.decode(encoded.input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eaf06a18-6bba-4239-9820-c5c9dda28712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 1005, 1049, 2200, 2204, 2012, 2394, 102, 1045, 1005, 1049, 2025, 2204, 1999, 3009, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer(\"I'm very good at english\", \"I'm not good in spanish\")\n",
    "encoded # una coppia di frasi x sentence similarity (se le 2 frasi sono consequenziali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d010b416-afe3-4390-ad94-6569af8380e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.33.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "config # parametri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f692b3-1be7-49a3-86c4-37d459f21161",
   "metadata": {},
   "source": [
    "#### Encoding del Testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d86a890-49a5-4827-a380-711e53b414e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 1005, 1049, 3100, 102, 7483, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"I'm okay\", \"yesterday\")\n",
    "# token_type_ids si capisce meglio tokenizzando una coppia di frasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6acbabc-8c92-4e34-a4ba-745ddf77cc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87500be7-2f45-44e5-a90f-997a613d5bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████| 16000/16000 [00:02<00:00, 7418.17 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 7458.11 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 7100.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.reset_format()\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda x: tokenizer( x[\"text\"] )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80d76e03-9973-4867-a5bc-74efaa8e5ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_descr</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[101, 1045, 2134, 2102, 2514, 26608, 102]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[101, 1045, 2064, 2175, 2013, 3110, 2061, 2062...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_descr  \\\n",
       "0                            i didnt feel humiliated      0     sadness   \n",
       "1  i can go from feeling so hopeless to so damned...      0     sadness   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0          [101, 1045, 2134, 2102, 2514, 26608, 102]   \n",
       "1  [101, 1045, 2064, 2175, 2013, 3110, 2061, 2062...   \n",
       "\n",
       "                                      token_type_ids  \\\n",
       "0                              [0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0                              [1, 1, 1, 1, 1, 1, 1]  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.set_format(\"pandas\")\n",
    "dataset[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd3d9e83-badb-4b22-b93a-e2d4b3de3b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[101, 1045, 2134, 2102, 2514, 26608, 102]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[101, 1045, 2064, 2175, 2013, 3110, 2061, 2062...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                          input_ids  \\\n",
       "0      0          [101, 1045, 2134, 2102, 2514, 26608, 102]   \n",
       "1      0  [101, 1045, 2064, 2175, 2013, 3110, 2061, 2062...   \n",
       "\n",
       "                                      token_type_ids  \\\n",
       "0                              [0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0                              [1, 1, 1, 1, 1, 1, 1]  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = dataset[\"train\"]\n",
    "trainset = trainset.remove_columns([\"text\", \"label_descr\"])\n",
    "trainset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d72a3b7-28cb-4705-8c84-bf80c3aaf6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "65ad2135-c5cc-4a35-9602-7ea405f59b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_padding(batch):  # un batch è un dictionary. Keys: Values. label: [...] input_ids: [[...][...]]\n",
    "    # la funzione fa padding degli input_ids\n",
    "    max_len = max([len(seq[\"input_ids\"]) for seq in batch])\n",
    "    def pad(l):\n",
    "        return np.concatenate([l, [0]*(max_len-len(l))])\n",
    "\n",
    "    d = {}\n",
    "    d[\"label\"] = [el[\"label\"] for el in batch]\n",
    "    for key in ['input_ids', 'token_type_ids', 'attention_mask']:\n",
    "        d[key] = [pad(l[key]) for l in batch]\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6fefab1-f671-4955-945a-35140cd4adc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method to_tf_dataset in module datasets.arrow_dataset:\n",
      "\n",
      "to_tf_dataset(batch_size: Optional[int] = None, columns: Union[str, List[str], NoneType] = None, shuffle: bool = False, collate_fn: Optional[Callable] = None, drop_remainder: bool = False, collate_fn_args: Optional[Dict[str, Any]] = None, label_cols: Union[str, List[str], NoneType] = None, prefetch: bool = True, num_workers: int = 0, num_test_batches: int = 20) method of datasets.arrow_dataset.Dataset instance\n",
      "    Create a `tf.data.Dataset` from the underlying Dataset. This `tf.data.Dataset` will load and collate batches from\n",
      "    the Dataset, and is suitable for passing to methods like `model.fit()` or `model.predict()`. The dataset will yield\n",
      "    `dicts` for both inputs and labels unless the `dict` would contain only a single key, in which case a raw\n",
      "    `tf.Tensor` is yielded instead.\n",
      "    \n",
      "    Args:\n",
      "        batch_size (`int`, *optional*):\n",
      "            Size of batches to load from the dataset. Defaults to `None`, which implies that the dataset won't be\n",
      "            batched, but the returned dataset can be batched later with `tf_dataset.batch(batch_size)`.\n",
      "        columns (`List[str]` or `str`, *optional*):\n",
      "            Dataset column(s) to load in the `tf.data.Dataset`.\n",
      "            Column names that are created by the `collate_fn` and that do not exist in the original dataset can be used.\n",
      "        shuffle(`bool`, defaults to `False`):\n",
      "            Shuffle the dataset order when loading. Recommended `True` for training, `False` for\n",
      "            validation/evaluation.\n",
      "        drop_remainder(`bool`, defaults to `False`):\n",
      "            Drop the last incomplete batch when loading. Ensures\n",
      "            that all batches yielded by the dataset will have the same length on the batch dimension.\n",
      "        collate_fn(`Callable`, *optional*):\n",
      "            A function or callable object (such as a `DataCollator`) that will collate\n",
      "            lists of samples into a batch.\n",
      "        collate_fn_args (`Dict`, *optional*):\n",
      "            An optional `dict` of keyword arguments to be passed to the\n",
      "            `collate_fn`.\n",
      "        label_cols (`List[str]` or `str`, defaults to `None`):\n",
      "            Dataset column(s) to load as labels.\n",
      "            Note that many models compute loss internally rather than letting Keras do it, in which case\n",
      "            passing the labels here is optional, as long as they're in the input `columns`.\n",
      "        prefetch (`bool`, defaults to `True`):\n",
      "            Whether to run the dataloader in a separate thread and maintain\n",
      "            a small buffer of batches for training. Improves performance by allowing data to be loaded in the\n",
      "            background while the model is training.\n",
      "        num_workers (`int`, defaults to `0`):\n",
      "            Number of workers to use for loading the dataset. Only supported on Python versions >= 3.8.\n",
      "        num_test_batches (`int`, defaults to `20`):\n",
      "            Number of batches to use to infer the output signature of the dataset.\n",
      "            The higher this number, the more accurate the signature will be, but the longer it will take to\n",
      "            create the dataset.\n",
      "    \n",
      "    Returns:\n",
      "        `tf.data.Dataset`\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "    ```py\n",
      "    >>> ds_train = ds[\"train\"].to_tf_dataset(\n",
      "    ...    columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      "    ...    shuffle=True,\n",
      "    ...    batch_size=16,\n",
      "    ...    collate_fn=data_collator,\n",
      "    ... )\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(trainset.to_tf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40daa3c6-b2ba-43c5-bedf-b79e0b126e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trasforma in Dataset Tensorflow\n",
    "\n",
    "trainset_tf = trainset.to_tf_dataset(\n",
    "    collate_fn=collate_padding, # funzione che costruisce batch da singoli sample\n",
    "    batch_size=32, label_cols=\"label\", shuffle=True, columns=['input_ids', 'token_type_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8cd8bce9-a777-437f-b13f-3b780da9017e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': <tf.Tensor: shape=(32, 40), dtype=float32, numpy=\n",
       "  array([[  101.,  1045.,  2018., ...,     0.,     0.,     0.],\n",
       "         [  101.,  1045.,  2514., ...,     0.,     0.,     0.],\n",
       "         [  101.,  1045.,  2123., ...,     0.,     0.,     0.],\n",
       "         ...,\n",
       "         [  101.,  1045.,  2228., ...,     0.,     0.,     0.],\n",
       "         [  101., 10047.,  2145., ...,     0.,     0.,     0.],\n",
       "         [  101.,  5665.,  2022., ...,     0.,     0.,     0.]],\n",
       "        dtype=float32)>,\n",
       "  'token_type_ids': <tf.Tensor: shape=(32, 40), dtype=float32, numpy=\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       "  'attention_mask': <tf.Tensor: shape=(32, 40), dtype=float32, numpy=\n",
       "  array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.]], dtype=float32)>},\n",
       " <tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
       " array([3, 0, 1, 1, 0, 1, 0, 0, 4, 0, 1, 1, 3, 1, 1, 1, 2, 3, 0, 1, 5, 3,\n",
       "        1, 4, 4, 0, 0, 4, 4, 0, 4, 1])>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(trainset_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9bea1289-179a-4941-b006-5358819723a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertForSequenceClassification' object has no attribute 'compile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m(keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;241m1e-5\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/MachineLearning/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertForSequenceClassification' object has no attribute 'compile'"
     ]
    }
   ],
   "source": [
    "model.compile(keras.optimizers.Adam(1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcbee8b-1f55-460b-96b2-38bec99a1ca0",
   "metadata": {},
   "source": [
    "## HF file Prof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a8084-6705-46a2-b185-d0b73cb40d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "dataset = datasets.load_dataset(path=\"emotion\")\n",
    "\n",
    "dataset\n",
    "\n",
    "dataset[\"train\"][0]\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "train_dataset.features\n",
    "\n",
    "train_dataset[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7973ba40-a5a8-4917-838d-ed7ca1546083",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_format(\"pandas\")\n",
    "\n",
    "train_dataset[:4]\n",
    "\n",
    "dataset.reset_format()\n",
    "\n",
    "# label_str\n",
    "dataset = dataset.map(lambda x: dict(label_descr=train_dataset.features[\"label\"].int2str(x[\"label\"])))\n",
    "\n",
    "dataset.set_format(\"pandas\")\n",
    "\n",
    "dataset[\"train\"][:5]\n",
    "\n",
    "model = TFAutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=6)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "encoded = tokenizer(\"I'm very good at english\", \"I'm not good in spanish\")\n",
    "\n",
    "encoded\n",
    "\n",
    "tokens = tokenizer.decode(encoded.input_ids)\n",
    "\n",
    "tokens\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "config\n",
    "\n",
    "dataset[\"train\"][:2]\n",
    "\n",
    "tokenizer(\"I'm ok\", \"yesterday\")\n",
    "\n",
    "tokenizer.model_input_names\n",
    "\n",
    "dataset[\"train\"][:2]\n",
    "\n",
    "dataset.reset_format()\n",
    "\n",
    "dataset = dataset.map(lambda x: tokenizer(x[\"text\"]))\n",
    "\n",
    "trainset[:2]\n",
    "\n",
    "dataset.set_format(\"pandas\")\n",
    "\n",
    "trainset = dataset[\"train\"]\n",
    "\n",
    "trainset = trainset.remove_columns([\"text\", \"label_descr\"])\n",
    "\n",
    "trainset[:2]\n",
    "\n",
    "list(trainset.features.keys())\n",
    "\n",
    "trainset[:4].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0408c7-61bb-4f1a-9757-d320f5e29774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def collate_padding(batch): # prende un set di sample e li trasforma in un batch\n",
    "    # batch = lista di record (come dizionari)\n",
    "    max_len = max([len(seq[\"input_ids\"]) for seq in batch])\n",
    "    def pad(l):\n",
    "        return np.concatenate([l, [0]*(max_len-len(l))])\n",
    "\n",
    "    d = {}\n",
    "    d[\"label\"] = [el[\"label\"] for el in batch]\n",
    "    for key in ['input_ids', 'attention_mask']:\n",
    "        d[key] = [pad(l[key]).astype(np.int32) for l in batch]\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8260028-6508-4c18-8b65-31e355efd838",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_padding([dataset[\"train\"][i] for i in range(2)]) # nota la attention mask delle due frasi (con gli zeri: c'è padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6a4cca-298e-46f5-9e3f-e8a2667bdf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(trainset.to_tf_dataset)\n",
    "\n",
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ceb41-7695-4504-bb64-d1070c673510",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_tf = dataset[\"train\"].to_tf_dataset(\n",
    "    collate_fn=collate_padding, \n",
    "    batch_size=32, label_cols=\"label\", shuffle=True, columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e5820-0fe6-4a45-b01c-83df3250dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "valset_tf = dataset[\"validation\"].to_tf_dataset(\n",
    "    collate_fn=collate_padding, \n",
    "    batch_size=32, label_cols=\"label\", shuffle=True, columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8980123e-e55b-4b9f-ac11-d949fcfd5f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_tf = dataset[\"test\"].to_tf_dataset(\n",
    "    collate_fn=collate_padding, \n",
    "    batch_size=32, label_cols=\"label\", shuffle=False, columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b887d-ccee-4728-af41-41089e0aecd4",
   "metadata": {},
   "source": [
    "**Nota**: classi per caricare/usare dataset (i dataset vengono caricati dinamicamente)\n",
    "+ tf.data.Dataset\n",
    "+ keras.Sequence\n",
    "+ in alternativa: fare un generatore\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b39bbc-59a6-4d86-9330-5293948af0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(trainset_tf))\n",
    "\n",
    "# in alternativa\n",
    "for b in trainset_tf:\n",
    "    print(b)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1563103-41e9-4017-abbd-2009a187f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(keras.optimizers.Adam(1e-5), metrics=[\"accuracy\"])\n",
    "# learning rate basso perché è un finetuning, fare aggiustamenti piccoli. altrimenti modifica il body di BERT che si vuole invece preservare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb787c-311c-4961-af92-8ecc7e4e86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=trainset_tf, validation_data=valset_tf, epochs=1) # validation_split=0.2 si può fare solo se passo tensori. con dataset e generatori no, x=trainset contiene già feature e label\n",
    "# però gli passo il validation_data: valset_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc837a3-5e3c-473f-81a8-ca7e235203c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ecb7ef-de66-40ec-8bbe-ff62a6de9017",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(testset_tf, batch_size = 32)\n",
    "preds.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947dc548-ce42-419c-b18c-f60d9d62f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32849902-190c-4f7d-8a76-e18e4933720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = np.array(preds.logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b1854-5310-4dda-84a7-da406eace7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f571d8-66b3-4b5a-b055-b7d615edd011",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_classes = np.array(dataset[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff6e959-c650-46c1-9d1b-d7766593d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcolo accuracy\n",
    "accuracy_score(real_classes, predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0186582f-77a1-4672-b109-69c52039b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=dataset[\"train\"].features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c85303-83ed-4887-9cfb-6ab841aa637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(real_classes, predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6696dad7-fc3d-4e2b-83a6-d188efdbf963",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(cm, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
