{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md\n",
        "!python -m spacy download it_core_news_md"
      ],
      "metadata": {
        "id": "ngFxGdJprQKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from tensorflow.data import Dataset\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import GRUCell, Embedding, Attention\n",
        "import json"
      ],
      "metadata": {
        "id": "62Ap-HTCpyMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ps6Q2QbnmORQ",
        "outputId": "4d541dec-dc30-483d-96c9-9fad0484d3ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-14 20:22:15--  https://www.manythings.org/anki/ita-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8220355 (7.8M) [application/zip]\n",
            "Saving to: ‘ita-eng.zip’\n",
            "\n",
            "ita-eng.zip         100%[===================>]   7.84M  17.9MB/s    in 0.4s    \n",
            "\n",
            "2023-09-14 20:22:16 (17.9 MB/s) - ‘ita-eng.zip’ saved [8220355/8220355]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.manythings.org/anki/ita-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ita-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3KENBgWo2Jz",
        "outputId": "4ee1ffca-f0c7-44b4-a73d-a4128525ba97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ita-eng.zip\n",
            "  inflating: ita.txt                 \n",
            "  inflating: _about.txt              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "UNK = \"<UNK>\"\n",
        "BOS = \"<BOS>\"\n",
        "EOS = \"<EOS>\"\n",
        "PAD = \"<PAD>\""
      ],
      "metadata": {
        "id": "-wesJLDoVWX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX = 0\n",
        "UNK_IDX = 1\n",
        "BOS_IDX = 2\n",
        "EOS_IDX = 3"
      ],
      "metadata": {
        "id": "bpyzYDyt3dq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tokenizzazione"
      ],
      "metadata": {
        "id": "U4tZPWUFpqdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, language):\n",
        "        self.language = language\n",
        "        if language == \"it\":\n",
        "            self.nlp = spacy.load(\"it_core_news_md\")\n",
        "        else:\n",
        "            self.nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return [token.text.lower() for token in self.nlp(text)]"
      ],
      "metadata": {
        "id": "aw9S760lprjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# vocabolario"
      ],
      "metadata": {
        "id": "tyZ2ZBjAr-cR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabularies(filepath, num_rows, vocab_size):\n",
        "\n",
        "    # counter\n",
        "    en_counter = Counter()\n",
        "    it_counter = Counter()\n",
        "\n",
        "    # tokenizzatori\n",
        "    en_tokenizer = Tokenizer(\"en\")\n",
        "    it_tokenizer = Tokenizer(\"it\")\n",
        "\n",
        "    # vocabolari\n",
        "    en_vocab = {\"<PAD>\": 0, \"<UNK>\": 1, \"<BOS>\": 2, \"<EOS>\": 3}\n",
        "    it_vocab = {\"<PAD>\": 0, \"<UNK>\": 1, \"<BOS>\": 2, \"<EOS>\": 3}\n",
        "\n",
        "    counter = 0\n",
        "    pbar = tqdm()\n",
        "    with open(filepath, \"r\") as f:\n",
        "        while (line := f.readline()) is not None:\n",
        "            en_sentence, it_sentence, *_ = line.split(\"\\t\")\n",
        "            en_tokenized = en_tokenizer.tokenize(en_sentence)\n",
        "            it_tokenized = it_tokenizer.tokenize(it_sentence)\n",
        "\n",
        "            # update dei Counter\n",
        "            en_counter.update(en_tokenized)\n",
        "            it_counter.update(it_tokenized)\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "            counter += 1\n",
        "            if num_rows and counter >= num_rows:\n",
        "                break\n",
        "\n",
        "    # update dei dizionari\n",
        "    most_common_en = [item[0] for item in en_counter.most_common(vocab_size)]\n",
        "    most_common_it = [item[0] for item in it_counter.most_common(vocab_size)]\n",
        "\n",
        "    en_vocab |= {word: idx for idx, word in enumerate(most_common_en, start=4)}\n",
        "    it_vocab |= {word: idx for idx, word in enumerate(most_common_it, start=4)}\n",
        "\n",
        "    en_vocab_inv = {idx: word for word, idx in en_vocab.items()}\n",
        "    it_vocab_inv = {idx: word for word, idx in it_vocab.items()}\n",
        "\n",
        "    return en_vocab, en_vocab_inv, it_vocab, it_vocab_inv\n",
        "\n"
      ],
      "metadata": {
        "id": "aCcyydqAsEBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_vocab, en_vocab_inv, it_vocab, it_vocab_inv = build_vocabularies(filepath=\"ita.txt\", num_rows=100_000, vocab_size=10_000)"
      ],
      "metadata": {
        "id": "oX04zDv_v0HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dump vocabularies"
      ],
      "metadata": {
        "id": "3GTiO_3AX4Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"en_vocab.json\", \"w\") as f:\n",
        "    json.dump(en_vocab, f)\n",
        "\n",
        "with open(\"en_vocab_inv.json\", \"w\") as f:\n",
        "    json.dump(en_vocab_inv, f)\n",
        "\n",
        "with open(\"it_vocab.json\", \"w\") as f:\n",
        "    json.dump(it_vocab, f)\n",
        "\n",
        "with open(\"it_vocab_inv.json\", \"w\") as f:\n",
        "    json.dump(it_vocab_inv, f)"
      ],
      "metadata": {
        "id": "2rZ0BuxBYGUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load vocabularies"
      ],
      "metadata": {
        "id": "Usi1BDxCX6d7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"en_vocab.json\") as f:\n",
        "    en_vocab = json.load(f)\n",
        "\n",
        "with open(\"en_vocab_inv.json\") as f:\n",
        "    en_vocab_inv = json.load(f)\n",
        "\n",
        "with open(\"it_vocab.json\") as f:\n",
        "    it_vocab = json.load(f)\n",
        "\n",
        "with open(\"it_vocab_inv.json\") as f:\n",
        "    it_vocab_inv = json.load(f)"
      ],
      "metadata": {
        "id": "PyVlzwQQ-GDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "it_vocab_inv = {int(k): v for k, v in it_vocab_inv.items()}\n",
        "en_vocab_inv = {int(k): v for k, v in en_vocab_inv.items()}"
      ],
      "metadata": {
        "id": "IRTQP0A50T8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# split train-validation-test"
      ],
      "metadata": {
        "id": "ap3gBed41Ysh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_file(filepath):\n",
        "    with open(filepath, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    num_rows = len(lines)\n",
        "    shuffled_row_idxs = random.sample(list(range(num_rows)), k=num_rows)\n",
        "\n",
        "    train_idxs = shuffled_row_idxs[:int(num_rows*0.6)]\n",
        "    val_idxs = shuffled_row_idxs[int(num_rows*0.6):int(num_rows*0.8)]\n",
        "    test_idxs = shuffled_row_idxs[int(num_rows*0.8):]\n",
        "\n",
        "    # train\n",
        "    with open(\"train.txt\", \"w\") as f:\n",
        "        f.writelines([lines[idx] for idx in train_idxs])\n",
        "\n",
        "    # validation\n",
        "    with open(\"val.txt\", \"w\") as f:\n",
        "        f.writelines([lines[idx] for idx in val_idxs])\n",
        "\n",
        "    # test\n",
        "    with open(\"test.txt\", \"w\") as f:\n",
        "        f.writelines([lines[idx] for idx in test_idxs])"
      ],
      "metadata": {
        "id": "AIRsEu7c1j1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_file(\"ita.txt\")"
      ],
      "metadata": {
        "id": "nANyIJ533-lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generatori"
      ],
      "metadata": {
        "id": "G0kWmX3b-C-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_generator(filepath, en_dict, it_dict, en_tokenizer, it_tokenizer):\n",
        "    def gen():\n",
        "        with open(filepath, \"r\") as f:\n",
        "            while (line := f.readline()) is not None:\n",
        "                en_sentence, it_sentence, *_ = line.split(\"\\t\")\n",
        "                en_sentence_tokenized = en_tokenizer.tokenize(en_sentence)\n",
        "                it_sentence_tokenized = it_tokenizer.tokenize(it_sentence)\n",
        "                src_sentence_tokenized = en_sentence_tokenized + [EOS]\n",
        "                tgt_sentence_in_tokenized = [BOS] + it_sentence_tokenized\n",
        "                tgt_sentence_out_tokenized = it_sentence_tokenized.copy() + [EOS]\n",
        "\n",
        "                src_sentence_encoded = [en_dict.get(token, en_dict[UNK]) for token in src_sentence_tokenized]\n",
        "                tgt_sentence_in_encoded = [it_dict.get(token, it_dict[UNK]) for token in tgt_sentence_in_tokenized]\n",
        "                tgt_sentence_out_encoded = [it_dict.get(token, it_dict[UNK]) for token in tgt_sentence_out_tokenized]\n",
        "\n",
        "                yield (src_sentence_encoded, tgt_sentence_in_encoded), tgt_sentence_out_encoded\n",
        "\n",
        "    return gen"
      ],
      "metadata": {
        "id": "Xahh-257-QKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for (src, tgt_in), tgt_out in dataset_generator(\"val.txt\", en_vocab, it_vocab, Tokenizer(\"en\"), Tokenizer(\"it\")):\n",
        "#     print(src)\n",
        "#     print(tgt_in)\n",
        "#     print(tgt_out)\n",
        "#     break"
      ],
      "metadata": {
        "id": "wdd7bvBmYWjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = Dataset.from_generator(\n",
        "    generator=dataset_generator(\"train.txt\", en_vocab, it_vocab, Tokenizer(\"en\"), Tokenizer(\"it\")),\n",
        "    output_signature=(\n",
        "        (tf.TensorSpec(shape=(None,), dtype=tf.int32), tf.TensorSpec(shape=(None,), dtype=tf.int32)),\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.int32))\n",
        ")\n",
        "\n",
        "valset = Dataset.from_generator(\n",
        "    generator=dataset_generator(\"val.txt\", en_vocab, it_vocab, Tokenizer(\"en\"), Tokenizer(\"it\")),\n",
        "    output_signature=(\n",
        "        (tf.TensorSpec(shape=(None,), dtype=tf.int32), tf.TensorSpec(shape=(None,), dtype=tf.int32)),\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.int32))\n",
        ")\n",
        "\n",
        "testset = Dataset.from_generator(\n",
        "    generator=dataset_generator(\"test.txt\", en_vocab, it_vocab, Tokenizer(\"en\"), Tokenizer(\"it\")),\n",
        "    output_signature=(\n",
        "        (tf.TensorSpec(shape=(None,), dtype=tf.int32), tf.TensorSpec(shape=(None,), dtype=tf.int32)),\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.int32))\n",
        ")"
      ],
      "metadata": {
        "id": "lpgWbHFvYxDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = trainset.shuffle(buffer_size=1000, reshuffle_each_iteration=True)\n",
        "trainset = trainset.padded_batch(batch_size=32)"
      ],
      "metadata": {
        "id": "SCWAwZtFbio_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoderDecoder:\n",
        "    def __init__(self, en_vocab, en_vocab_inv, it_vocab, it_vocab_inv):\n",
        "        self.en_vocab = en_vocab\n",
        "        self.en_vocab_inv = en_vocab_inv\n",
        "        self.it_vocab = it_vocab\n",
        "        self.it_vocab_inv = it_vocab_inv\n",
        "        self.nlp_it = spacy.load(\"it_core_news_md\")\n",
        "        self.nlp_en = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "    def encode(self, text, language):\n",
        "        vocab = self.en_vocab if language == \"en\" else self.it_vocab\n",
        "        nlp = self.nlp_en if language == \"en\" else self.nlp_it\n",
        "        tokenized_text = [token.text.lower() for token in nlp(text)]\n",
        "        return [vocab.get(token, vocab[UNK]) for token in tokenized_text]\n",
        "\n",
        "    def decode(self, coded_text, language):\n",
        "        vocab_inv = self.en_vocab_inv if language == \"en\" else self.it_vocab_inv\n",
        "        return [vocab_inv[code] for code in coded_text]"
      ],
      "metadata": {
        "id": "EklA1mMxwMae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modello"
      ],
      "metadata": {
        "id": "O7SO2N9jf7tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(keras.Model):\n",
        "    def __init__(self, vocabulary_size, embedding_size, recurrent_layers, recurrent_units, **kwargs):\n",
        "        \"\"\"\n",
        "        args\n",
        "        ----\n",
        "        - vocabulary_size (int): including special tokens (<BOS>, <EOS>, <UNK>)\n",
        "        - embedding_size (int): dimensione dello spazio degli embedding\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # embedding\n",
        "        # 0 index mean padding\n",
        "        self.embedding = Embedding(\n",
        "            vocabulary_size, embedding_size, mask_zero=True)\n",
        "\n",
        "        gru_cells = [GRUCell(recurrent_units) for _ in range(recurrent_layers)]\n",
        "\n",
        "        stacked_cells = tf.keras.layers.StackedRNNCells(gru_cells)\n",
        "        self.gru_layer = tf.keras.layers.RNN(stacked_cells, return_state=True, return_sequences=True)\n",
        "\n",
        "    def call(self, data, training=None):\n",
        "        x = self.embedding(data, training=training)\n",
        "        output, *state = self.gru_layer(x, training=training)\n",
        "\n",
        "        return output, state"
      ],
      "metadata": {
        "id": "-pyk5t36rbk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(keras.Model):\n",
        "    def __init__(self, vocabulary_size, embedding_size, recurrent_layers, recurrent_units, attention=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.recurrent_layers = recurrent_layers\n",
        "        self.recurrent_units = recurrent_units\n",
        "\n",
        "        self.embedding = Embedding(\n",
        "            vocabulary_size, embedding_size, mask_zero=True)\n",
        "\n",
        "        gru_cells = [GRUCell(recurrent_units) for _ in range(recurrent_layers)]\n",
        "\n",
        "        stacked_cells = tf.keras.layers.StackedRNNCells(gru_cells)\n",
        "        self.gru_layer = tf.keras.layers.RNN(stacked_cells, return_sequences=True, return_state=True)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(vocabulary_size)\n",
        "\n",
        "        if attention:\n",
        "            self.attention = Attention(score_mode=\"dot\")\n",
        "\n",
        "    def call(self, target_in, encoder_output, encoder_state, training=None, max_sentence_length=None):\n",
        "        # target_in.shape = batch x length\n",
        "        # initial_state.shape = batch x recurrent_layers x recurrent_units\n",
        "        # encoder_output.shape = batch x length x recurrent_units\n",
        "\n",
        "        # Addestramento\n",
        "        x = self.embedding(target_in, training=training)\n",
        "        # data.shape = batch x length x embedding\n",
        "        # concatena il contesto con l'input\n",
        "        # x = tf.concat([x, tf.repeat(tf.expand_dims(encoder_output[:, -1, :], axis=1), repeats=x.shape[1], axis=1)], axis=2)\n",
        "\n",
        "        output, *state = self.gru_layer(x, training=training, initial_state=encoder_state)\n",
        "        return tf.keras.activations.softmax(self.dense(output)), state\n",
        "\n",
        "    def generate(self, encoder_output, encoder_state, training=None, max_sentence_length=None):\n",
        "        # Generazione\n",
        "        batch_size = encoder_output.shape[0]\n",
        "        x = tf.fill([batch_size, 1], BOS_IDX)\n",
        "        state = encoder_state\n",
        "        out_words_list = []\n",
        "        for _ in range(max_sentence_length):\n",
        "            # x.shape = batch_size x 1 x embedding_size\n",
        "            x = self.embedding(x, training=training)\n",
        "            # aggiunge il contesto\n",
        "            # x = tf.concat([x, tf.expand_dims(encoder_output[:, -1, :], axis=1)], axis=2)\n",
        "            output, *state = self.gru_layer(x, training=training, initial_state=state)\n",
        "            # output_size = batch_size x 1 x embedding_size\n",
        "\n",
        "            # trova i caratteri più probabili\n",
        "            # probs.shape = batch_size x 1 x vocabulary_size\n",
        "            probs = tf.keras.activations.softmax(self.dense(output))\n",
        "            x = tf.argmax(probs, axis=-1)\n",
        "            out_words_list.append(x.numpy().item())\n",
        "\n",
        "        return out_words_list"
      ],
      "metadata": {
        "id": "rG0MTvBGrxz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SENTENCE_LENGTH = 20"
      ],
      "metadata": {
        "id": "IyqDnFlc3Cpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(keras.Model):\n",
        "    def __init__(self, vocabulary_size, embedding_size, recurrent_layers, recurrent_units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.recurrent_layers = recurrent_layers\n",
        "        self.recurrent_units = recurrent_units\n",
        "\n",
        "\n",
        "        self.encoder = Encoder(vocabulary_size, embedding_size,\n",
        "                               recurrent_layers, recurrent_units)\n",
        "        self.decoder = Decoder(vocabulary_size, embedding_size, recurrent_layers,\n",
        "                               recurrent_units)\n",
        "\n",
        "\n",
        "    def call(self, data, training=None, max_sentence_length=MAX_SENTENCE_LENGTH):\n",
        "        # unpack data\n",
        "        src_sentences, dst_sentences = data\n",
        "\n",
        "        # encoder call\n",
        "        # encoder_output.shape = batch x len_sentences x encoder_recurrent_units\n",
        "        encoder_output, encoder_state = self.encoder(src_sentences, training=training)\n",
        "\n",
        "        decoder_output, decoder_state = self.decoder(dst_sentences, encoder_output, encoder_state, training=training, max_sentence_length=max_sentence_length)\n",
        "\n",
        "        return decoder_output\n",
        "\n",
        "    def generate(self, data, training=None, max_sentence_length=MAX_SENTENCE_LENGTH):\n",
        "        # unpack data\n",
        "        src_sentences = data\n",
        "\n",
        "        # encoder call\n",
        "        # encoder_output.shape = batch x len_sentences x encoder_recurrent_units\n",
        "        encoder_output, encoder_state = self.encoder(src_sentences, training=training)\n",
        "\n",
        "        words = self.decoder.generate(encoder_output, encoder_state, training=training, max_sentence_length=max_sentence_length)\n",
        "\n",
        "        return words"
      ],
      "metadata": {
        "id": "q6_9ew1pr0jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss(y_true, y_pred):\n",
        "    y_true_reshaped = tf.reshape(y_true, [-1])\n",
        "    y_pred_reshaped = tf.reshape(y_pred, [-1, y_pred.shape[-1]])\n",
        "    scc = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        reduction=tf.keras.losses.Reduction.NONE)\n",
        "    results = scc(y_true_reshaped, y_pred_reshaped)\n",
        "    mask = tf.cast(y_true_reshaped != 0, tf.float32)\n",
        "    return tf.reduce_sum(results*mask) / tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "XYBHzwjapKGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Addestramento"
      ],
      "metadata": {
        "id": "JdTs4kiC36CO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_decoder = EncoderDecoder(vocabulary_size=10_000+4, embedding_size=128, recurrent_layers=1, recurrent_units=128)"
      ],
      "metadata": {
        "id": "xcSDNyEv38uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_decoder.compile(optimizer=keras.optimizers.Adam(), loss=custom_loss, run_eagerly=False)"
      ],
      "metadata": {
        "id": "jYp2P-Pm-qCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_decoder.fit(x=trainset, steps_per_epoch=100, epochs=100, initial_epoch=71)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOGJLwh--2vC",
        "outputId": "f03c5237-d05e-4709-aa7b-89cfed7d7e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 72/100\n",
            "100/100 [==============================] - 68s 541ms/step - loss: 1.9064\n",
            "Epoch 73/100\n",
            "100/100 [==============================] - 57s 571ms/step - loss: 1.9124\n",
            "Epoch 74/100\n",
            "100/100 [==============================] - 54s 541ms/step - loss: 1.8792\n",
            "Epoch 75/100\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 1.9470\n",
            "Epoch 76/100\n",
            "100/100 [==============================] - 55s 547ms/step - loss: 2.0461\n",
            "Epoch 77/100\n",
            "100/100 [==============================] - 54s 544ms/step - loss: 2.0240\n",
            "Epoch 78/100\n",
            "100/100 [==============================] - 54s 542ms/step - loss: 2.0119\n",
            "Epoch 79/100\n",
            "100/100 [==============================] - 56s 558ms/step - loss: 1.9586\n",
            "Epoch 80/100\n",
            "100/100 [==============================] - 54s 543ms/step - loss: 1.9464\n",
            "Epoch 81/100\n",
            "100/100 [==============================] - 55s 550ms/step - loss: 1.9473\n",
            "Epoch 82/100\n",
            "100/100 [==============================] - 54s 542ms/step - loss: 2.0492\n",
            "Epoch 83/100\n",
            "100/100 [==============================] - 55s 546ms/step - loss: 2.0509\n",
            "Epoch 84/100\n",
            "100/100 [==============================] - 54s 544ms/step - loss: 2.0464\n",
            "Epoch 85/100\n",
            "100/100 [==============================] - 55s 546ms/step - loss: 2.0415\n",
            "Epoch 86/100\n",
            "100/100 [==============================] - 55s 549ms/step - loss: 1.9825\n",
            "Epoch 87/100\n",
            "100/100 [==============================] - 55s 547ms/step - loss: 1.9727\n",
            "Epoch 88/100\n",
            "100/100 [==============================] - 54s 537ms/step - loss: 2.0010\n",
            "Epoch 89/100\n",
            "100/100 [==============================] - 54s 538ms/step - loss: 1.9459\n",
            "Epoch 90/100\n",
            "100/100 [==============================] - 55s 549ms/step - loss: 1.9722\n",
            "Epoch 91/100\n",
            "100/100 [==============================] - 55s 546ms/step - loss: 2.0404\n",
            "Epoch 92/100\n",
            "100/100 [==============================] - 54s 539ms/step - loss: 2.0433\n",
            "Epoch 93/100\n",
            "100/100 [==============================] - 56s 556ms/step - loss: 1.9937\n",
            "Epoch 94/100\n",
            "100/100 [==============================] - 54s 536ms/step - loss: 2.0184\n",
            "Epoch 95/100\n",
            "100/100 [==============================] - 53s 534ms/step - loss: 2.0085\n",
            "Epoch 96/100\n",
            "100/100 [==============================] - 53s 532ms/step - loss: 1.9741\n",
            "Epoch 97/100\n",
            "100/100 [==============================] - 53s 528ms/step - loss: 1.9564\n",
            "Epoch 98/100\n",
            "100/100 [==============================] - 52s 524ms/step - loss: 1.9449\n",
            "Epoch 99/100\n",
            "100/100 [==============================] - 54s 541ms/step - loss: 1.9679\n",
            "Epoch 100/100\n",
            "100/100 [==============================] - 52s 524ms/step - loss: 1.9709\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f7c1a76a470>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_decoder.save_weights(\"weights_100.h5\")"
      ],
      "metadata": {
        "id": "eKPQq5eloQwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text generation"
      ],
      "metadata": {
        "id": "5Pk6iNeGhZGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"i am a good guy and my home is beautiful\"\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "tokens = [t.text.lower() for t in nlp(sentence)]\n",
        "idxs = [en_vocab.get(t, en_vocab[UNK]) for t in tokens]\n",
        "translation = encoder_decoder.generate(tf.reshape(tf.constant(idxs), [1, -1]))\n",
        "print(\" \".join([it_vocab_inv[idx] for idx in translation]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvdCSGuS-89a",
        "outputId": "7a0a0016-ecfb-4db1-a5e2-d3e2305a9eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sono un insegnante di francese , non sono bravo in un amico . <EOS> . <EOS> . <EOS> . <EOS>\n"
          ]
        }
      ]
    }
  ]
}